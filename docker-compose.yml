services:
  ollama:
    image: ollama/ollama
    container_name: ollama-container
    ports:
      - "11434:11434"
    volumes:
      - models-volume:/root/.ollama/models # Monta un volumen donde se guardarán los modelos
    networks:
      - shared-network

  model-puller:
    image: docker
    container_name: model-puller
    depends_on:
      - ollama
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - models-volume:/root/.ollama/models # Monta el mismo volumen
    networks:
      - shared-network
    entrypoint: ["sh", "-c", "
      if [ ! -f /root/.ollama/models/nomic-embed-text ]; then 
        docker exec ollama-container ollama pull nomic-embed-text && 
        echo 'nomic-embed-text completed'; 
      else 
        echo 'Model nomic-embed-text already exists'; 
      fi
    "]

  fastapi-app:
    build:
      context: .  # Usa tu Dockerfile ubicado en el directorio actual
    container_name: fastapi-app
    ports:
      - "8000:8000"  # Puerto para tu aplicación FastAPI
    depends_on:
      - ollama  # Asegura que 'ollama' esté listo antes de correr FastAPI
    volumes:
      - ./app:/code/app
      - ./files:/code/files
      - ./client_secrets.json:/code/client_secrets.json
      - ./credentials.json:/code/credentials.json
      - ./:/code  # Para otros archivos que estés usando
      - chroma-db-volume:/code/app/database/chroma   # Volumen para Chroma
      - sqlite3-db-volume:/code/app/database/sqlite3 # Volumen para SQLite3
      - questions-volume:/code/app/generator/q&as    # Volumen para las preguntas
    networks:
      - shared-network
    environment:
      OLLAMA_URL: http://ollama-container:11434

volumes:
  models-volume:
    driver: local
  chroma-db-volume:        # Volumen persistente para Chroma
    driver: local
  sqlite3-db-volume:       # Volumen persistente para SQLite3
    driver: local
  questions-volume:        # Volumen persistente para las preguntas
    driver: local

networks:
  shared-network:
    driver: bridge
